{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import ot\n",
    "from torch.autograd import Function\n",
    "\n",
    "# Define the Kullback-Leibler divergence\n",
    "def kl_divergence(p, q):\n",
    "    return (p * (p / q).log()).sum()\n",
    "\n",
    "# Define the entropy function\n",
    "def entropy(p):\n",
    "    return -(p * p.log()).sum()\n",
    "\n",
    "class SinkhornDistance(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, C, epsilon, max_iter):\n",
    "        # Sinkhorn iterations\n",
    "        K = torch.exp(-C.double() / epsilon)\n",
    "        u = torch.ones(C.shape[0]).double()\n",
    "        v = torch.ones(C.shape[1]).double()\n",
    "        \n",
    "        for _ in range(max_iter):\n",
    "            u = 1.0 / (K @ v)\n",
    "            v = 1.0 / (K.t() @ u)\n",
    "        \n",
    "        ctx.save_for_backward(K, u, v)\n",
    "        return torch.sum(u.reshape((-1, 1)) * C.double() * v.reshape((1, -1)) * K)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        K, u, v = ctx.saved_tensors\n",
    "        grad_C = grad_output * u.reshape((-1, 1)) * v.reshape((1, -1)) * K\n",
    "        return grad_C, None, None, None\n",
    "\n",
    "\n",
    "\n",
    "def optimize_unbalanced_ot(C, P, epsilon, tau1, tau2, w_g, w_f, max_iter=1000, lr=1e-2):\n",
    "    sinkhorn_distance = SinkhornDistance.apply\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.Adam([C], lr=lr)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute the Sinkhorn distance\n",
    "        dist = sinkhorn_distance(C,  epsilon, max_iter)\n",
    "\n",
    "        # Compute the Kullback-Leibler divergences and the entropy\n",
    "        kl1 = tau1 * kl_divergence(P.sum(1), w_g)\n",
    "        kl2 = tau2 * kl_divergence(P.sum(0), w_f)\n",
    "        ent = epsilon * entropy(P)\n",
    "\n",
    "        # Total loss\n",
    "        loss = dist - ent + kl1 + kl2\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_k_classes(C_optimized, P, k):\n",
    "    # Compute the measure of how much each class in the source domain contributes to the target domain\n",
    "    class_contributions = P.sum(axis=1)\n",
    "\n",
    "    # Select the indices of the top k values\n",
    "    #top_k_classes = np.argpartition(class_contributions, -k)[-k:]\n",
    "    top_k_classes = np.argsort(class_contributions)[:k]\n",
    "    return top_k_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([712, 640]) torch.Size([30, 640])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1062908/1315148984.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_g = torch.tensor(w_g)\n",
      "/tmp/ipykernel_1062908/1315148984.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_f = torch.tensor(w_f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 classes: [  0 469 470 471 472]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Step 1: Prepare your datasets\n",
    "\n",
    "target_list = torch.load('/home/raphael/Documents/models/B/f_baselinemetadataset_cub_test_features.pt')\n",
    "target_vectors = torch.stack([x['features'].mean(0) for x in target_list])\n",
    "source_vectors = torch.load('mean_original_imagenet.pt')\n",
    "print(source_vectors.shape, target_vectors.shape)\n",
    "num_classes_source = source_vectors.shape[0]\n",
    "num_classes_target = target_vectors.shape[0]\n",
    "\n",
    "# Calculate the cost matrix C\n",
    "C = np.zeros((num_classes_source, num_classes_target))\n",
    "for i in range(num_classes_source):\n",
    "    for j in range(num_classes_target):\n",
    "        C[i, j] = np.linalg.norm(source_vectors[i] - target_vectors[j])  # replace with actual distance metric\n",
    "\n",
    "# Convert to PyTorch tensor for optimization\n",
    "C = torch.tensor(C, requires_grad=True,dtype=torch.double)\n",
    "\n",
    "# Step 2: Normalize class distributions\n",
    "# For this example, we'll assume uniform distributions\n",
    "w_g = torch.ones(num_classes_source,dtype=torch.double) / num_classes_source  # replace with actual source class distribution\n",
    "w_f = torch.ones(num_classes_target,dtype=torch.double) / num_classes_target  # replace with actual target class distribution\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "w_g = torch.tensor(w_g)\n",
    "w_f = torch.tensor(w_f)\n",
    "\n",
    "# Assume we have a transportation matrix P\n",
    "# For this example, we'll just use a uniform matrix\n",
    "P = np.ones((num_classes_source, num_classes_target)) / num_classes_source*num_classes_target  # replace with actual initialization\n",
    "P = torch.tensor(P)\n",
    "\n",
    "# Parameters for optimization\n",
    "epsilon = 1.0  # entropy regularization parameter\n",
    "tau1 = 1.0  # parameter for KL divergence of source domain\n",
    "tau2 = 100.0  # parameter for KL divergence of target domain\n",
    "\n",
    "# Step 3: Run the optimization\n",
    "C_optimized = optimize_unbalanced_ot(C, P, epsilon, tau1, tau2, w_g, w_f)\n",
    "\n",
    "# Step 4: Select top k classes\n",
    "k = 5  # number of classes to select\n",
    "top_k_classes = select_top_k_classes(C_optimized.detach().numpy(), P.detach().numpy(), k)\n",
    "\n",
    "print(f'Top {k} classes: {top_k_classes}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = torch.load('name_original.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tench, Tinca tinca\n",
      "parallel bars, bars\n",
      "park bench\n",
      "passenger car, coach, carriage\n",
      "patio, terrace\n"
     ]
    }
   ],
   "source": [
    "for k in   [0 ,469, 470, 471, 472]:\n",
    "    print(name[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
